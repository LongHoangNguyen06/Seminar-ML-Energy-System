{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from pipeline import data\n",
    "from pipeline.config import CONF\n",
    "from pipeline.data import plots\n",
    "from pipeline.data import io\n",
    "from pipeline.data import inspection\n",
    "from pipeline.data import preprocess\n",
    "from pipeline.models import hyperopt\n",
    "from pipeline.models import training\n",
    "from pipeline.models.dataset import TimeSeriesDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pipeline.models.transformer import TimeSeriesTransformer\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# black is a code formatter (see https://github.com/psf/black).\n",
    "# It will automatically format the code you write in the cells imposing consistent Python style.\n",
    "%load_ext jupyter_black\n",
    "# matplotlib style file\n",
    "# Template for style file: https://matplotlib.org/stable/tutorials/introductory/customizing.html#customizing-with-style-sheets\n",
    "plt.style.use(\"../matplotlib_style.txt\")\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.expand_frame_repr\", False)  # Prevent wrapping\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data\n",
    "\n",
    "This takes about 1 minute for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data and not CONF.data.loaded_raw_data:\n",
    "    # Load raw data\n",
    "    (\n",
    "        Installed_Capacity_Germany_Raw,\n",
    "        Prices_Europe_Raw,\n",
    "        Realised_Supply_Germany_Raw,\n",
    "        Realised_Demand_Germany_Raw,\n",
    "        Weather_Data_Germany_Raw,\n",
    "        Weather_Data_Germany_2022_Raw,\n",
    "    ) = data.load_data(CONF=CONF)\n",
    "    CONF.data.loaded_raw_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    Installed_Capacity_Germany = Installed_Capacity_Germany_Raw.copy()\n",
    "    Prices_Europe = Prices_Europe_Raw.copy()\n",
    "    Realised_Supply_Germany = Realised_Supply_Germany_Raw.copy()\n",
    "    Realised_Demand_Germany = Realised_Demand_Germany_Raw.copy()\n",
    "    Weather_Data_Germany = Weather_Data_Germany_Raw.copy()\n",
    "    Weather_Data_Germany_2022 = Weather_Data_Germany_2022_Raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data and CONF.pipeline.inspect:\n",
    "    data.save_data_inspection(\n",
    "        Installed_Capacity_Germany=Installed_Capacity_Germany,\n",
    "        Prices_Europe=Prices_Europe,\n",
    "        Realised_Supply_Germany=Realised_Supply_Germany,\n",
    "        Realised_Demand_Germany=Realised_Demand_Germany,\n",
    "        Weather_Data_Germany=Weather_Data_Germany,\n",
    "        Weather_Data_Germany_2022=Weather_Data_Germany_2022,\n",
    "        CONF=CONF,\n",
    "        data_type=\"raw\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    inspection.date_range_and_resolution_dfs(\n",
    "        Installed_Capacity_Germany=Installed_Capacity_Germany,\n",
    "        Prices_Europe=Prices_Europe,\n",
    "        Realised_Supply_Germany=Realised_Supply_Germany,\n",
    "        Realised_Demand_Germany=Realised_Demand_Germany,\n",
    "        Weather_Data_Germany=Weather_Data_Germany,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.plot:\n",
    "    plots.plot_df(\n",
    "        Installed_Capacity_Germany,\n",
    "        \"Installed_Capacity_Germany\",\n",
    "        CONF,\n",
    "        processed_data=False,\n",
    "    )\n",
    "    plots.plot_df(Prices_Europe, \"Prices_Europe\", CONF, processed_data=False)\n",
    "    plots.plot_df(\n",
    "        Realised_Supply_Germany, \"Realised_Supply_Germany\", CONF, processed_data=False\n",
    "    )\n",
    "    plots.plot_df(\n",
    "        Realised_Demand_Germany, \"Realised_Demand_Germany\", CONF, processed_data=False\n",
    "    )\n",
    "    plots.plot_df(\n",
    "        Weather_Data_Germany,\n",
    "        \"Weather_Data_Germany\",\n",
    "        CONF,\n",
    "        date_col=io.DATE_COLUMNS_WEATHER[-1],\n",
    "        drop_date_cols=io.DATE_COLUMNS_WEATHER,\n",
    "        processed_data=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging weather data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    # Remove the data for 2022 from the original dataframe\n",
    "    Weather_Data_Germany = Weather_Data_Germany[\n",
    "        Weather_Data_Germany[\"time\"].dt.year != 2022\n",
    "    ]\n",
    "\n",
    "    # Concatenate the filtered original dataframe with the 2022 data\n",
    "    Weather_Data_Germany = pd.concat(\n",
    "        [Weather_Data_Germany, Weather_Data_Germany_2022], ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    Processed_Installed_Capacity_Germany = data.process_na_values(\n",
    "        Installed_Capacity_Germany, CONF\n",
    "    )\n",
    "    Processed_Prices_Europe = data.process_na_values(Prices_Europe, CONF)\n",
    "    Processed_Realised_Supply_Germany = data.process_na_values(\n",
    "        Realised_Supply_Germany, CONF\n",
    "    )\n",
    "    Processed_Realised_Demand_Germany = data.process_na_values(\n",
    "        Realised_Demand_Germany, CONF\n",
    "    )\n",
    "    Processed_Weather_Data_Germany = data.process_na_values(Weather_Data_Germany, CONF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to Greenwich time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    Processed_Realised_Supply_Germany = preprocess.german2greenwich(\n",
    "        Processed_Realised_Supply_Germany\n",
    "    )\n",
    "    Processed_Realised_Demand_Germany = preprocess.german2greenwich(\n",
    "        Processed_Realised_Demand_Germany\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregate weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    Processed_Weather_Data_Germany = preprocess.aggregate_weather_data(\n",
    "        Processed_Weather_Data_Germany, [\"forecast_origin\", \"time\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decrease demand and supply's time resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    Processed_Realised_Demand_Germany = Processed_Realised_Demand_Germany[\n",
    "        Processed_Realised_Demand_Germany[\"Date to\"].dt.minute == 0\n",
    "    ]\n",
    "    Processed_Realised_Demand_Germany[\"Date from\"] = Processed_Realised_Demand_Germany[\n",
    "        \"Date to\"\n",
    "    ] - pd.Timedelta(hours=1)\n",
    "    Processed_Realised_Supply_Germany = Processed_Realised_Supply_Germany[\n",
    "        Processed_Realised_Supply_Germany[\"Date to\"].dt.minute == 0\n",
    "    ]\n",
    "    Processed_Realised_Supply_Germany[\"Date from\"] = Processed_Realised_Supply_Germany[\n",
    "        \"Date to\"\n",
    "    ] - pd.Timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Increase time resolution of capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    new_row = {\n",
    "        \"Date from\": pd.Timestamp(\"2022-12-31 23:00:00\"),\n",
    "        \"Date to\": pd.Timestamp(\"2023-01-01 00:00:00\"),\n",
    "    }\n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    Processed_Installed_Capacity_Germany_hourly = pd.concat(\n",
    "        [Processed_Installed_Capacity_Germany, new_row_df], ignore_index=True\n",
    "    )\n",
    "\n",
    "    Processed_Installed_Capacity_Germany_hourly = (\n",
    "        Processed_Installed_Capacity_Germany_hourly.set_index(\"Date from\")\n",
    "    )\n",
    "    Processed_Installed_Capacity_Germany_hourly = (\n",
    "        Processed_Installed_Capacity_Germany_hourly.resample(\"H\").mean()\n",
    "    )\n",
    "    Processed_Installed_Capacity_Germany_hourly.reset_index(inplace=True)\n",
    "    Processed_Installed_Capacity_Germany_hourly[\"Date to\"] = (\n",
    "        Processed_Installed_Capacity_Germany_hourly[\"Date from\"] + pd.Timedelta(hours=1)\n",
    "    )\n",
    "    Processed_Installed_Capacity_Germany = Processed_Installed_Capacity_Germany_hourly\n",
    "    Processed_Installed_Capacity_Germany = Processed_Installed_Capacity_Germany.fillna(\n",
    "        method=\"ffill\"\n",
    "    )\n",
    "    inspection.date_range_and_resolution(\n",
    "        Processed_Installed_Capacity_Germany, io.DATE_COLUMNS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trim rows of every df to have same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    # trim first row of Processed_Weather_Data_Germany\n",
    "    Processed_Weather_Data_Germany = Processed_Weather_Data_Germany[\n",
    "        Processed_Weather_Data_Germany[\"time\"]\n",
    "        != Processed_Weather_Data_Germany[\"time\"].min()\n",
    "    ]\n",
    "\n",
    "    # trim last row of every other df\n",
    "    Processed_Installed_Capacity_Germany = Processed_Installed_Capacity_Germany[\n",
    "        Processed_Installed_Capacity_Germany[\"Date to\"]\n",
    "        != Processed_Installed_Capacity_Germany[\"Date to\"].max()\n",
    "    ]\n",
    "    Processed_Prices_Europe = Processed_Prices_Europe[\n",
    "        Processed_Prices_Europe[\"Date to\"] != Processed_Prices_Europe[\"Date to\"].max()\n",
    "    ]\n",
    "    Processed_Realised_Supply_Germany = Processed_Realised_Supply_Germany[\n",
    "        Processed_Realised_Supply_Germany[\"Date to\"]\n",
    "        != Processed_Realised_Supply_Germany[\"Date to\"].max()\n",
    "    ]\n",
    "    Processed_Realised_Demand_Germany = Processed_Realised_Demand_Germany[\n",
    "        Processed_Realised_Demand_Germany[\"Date to\"]\n",
    "        != Processed_Realised_Demand_Germany[\"Date to\"].max()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Patch time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    Processed_Prices_Europe = preprocess.patch_time_saving(Processed_Prices_Europe)\n",
    "    Processed_Realised_Demand_Germany = preprocess.patch_time_saving(\n",
    "        Processed_Realised_Demand_Germany\n",
    "    )\n",
    "    Processed_Realised_Supply_Germany = preprocess.patch_time_saving(\n",
    "        Processed_Realised_Supply_Germany\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data and CONF.pipeline.normalize_data:\n",
    "    print(\"Split data in train, val and test\")\n",
    "    (\n",
    "        Processed_Installed_Capacity_Germany,\n",
    "        Processed_Prices_Europe,\n",
    "        Processed_Realised_Supply_Germany,\n",
    "        Processed_Realised_Demand_Germany,\n",
    "        Processed_Weather_Data_Germany,\n",
    "    ) = preprocess.split_dfs(\n",
    "        Processed_Installed_Capacity_Germany,\n",
    "        Processed_Prices_Europe,\n",
    "        Processed_Realised_Supply_Germany,\n",
    "        Processed_Realised_Demand_Germany,\n",
    "        Processed_Weather_Data_Germany,\n",
    "    )\n",
    "\n",
    "    print(\"Normalizing data\")\n",
    "    (\n",
    "        Processed_Installed_Capacity_Germany,\n",
    "        Processed_Installed_Capacity_Germany_Scalers,\n",
    "    ) = preprocess.normalize_data(\n",
    "        df=Processed_Installed_Capacity_Germany,\n",
    "        ignore_features=io.DATE_COLUMNS,\n",
    "        constant=CONF.data.price_normalization_constant,\n",
    "    )\n",
    "\n",
    "    Processed_Prices_Europe, Processed_Prices_Europe_Scalers = (\n",
    "        preprocess.normalize_data(\n",
    "            df=Processed_Prices_Europe,\n",
    "            ignore_features=io.DATE_COLUMNS,\n",
    "            constant=CONF.data.price_normalization_constant,\n",
    "        )\n",
    "    )\n",
    "    Processed_Realised_Supply_Germany, Processed_Realised_Supply_Germany_Scalers = (\n",
    "        preprocess.normalize_data(\n",
    "            df=Processed_Realised_Supply_Germany, ignore_features=io.DATE_COLUMNS\n",
    "        )\n",
    "    )\n",
    "    Processed_Realised_Demand_Germany, Processed_Realised_Demand_Germany_Scalers = (\n",
    "        preprocess.normalize_data(\n",
    "            df=Processed_Realised_Demand_Germany, ignore_features=io.DATE_COLUMNS\n",
    "        )\n",
    "    )\n",
    "    Processed_Weather_Data_Germany, Processed_Weather_Data_Germany_Scalers = (\n",
    "        preprocess.normalize_data(\n",
    "            df=Processed_Weather_Data_Germany,\n",
    "            ignore_features=io.DATE_COLUMNS_WEATHER + [\"longitude\", \"latitude\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Remove train, test, val columns, again\")\n",
    "    Processed_Installed_Capacity_Germany = preprocess.remove_train_val_test_cols(\n",
    "        Processed_Installed_Capacity_Germany\n",
    "    )\n",
    "    Processed_Prices_Europe = preprocess.remove_train_val_test_cols(\n",
    "        Processed_Prices_Europe\n",
    "    )\n",
    "    Processed_Realised_Supply_Germany = preprocess.remove_train_val_test_cols(\n",
    "        Processed_Realised_Supply_Germany\n",
    "    )\n",
    "    Processed_Realised_Demand_Germany = preprocess.remove_train_val_test_cols(\n",
    "        Processed_Realised_Demand_Germany\n",
    "    )\n",
    "    Processed_Weather_Data_Germany = preprocess.remove_train_val_test_cols(\n",
    "        Processed_Weather_Data_Germany\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "    # Dictionary of scalers, each associated with a dataset\n",
    "    scalers = {\n",
    "        \"Processed_Installed_Capacity_Germany_Scalers\": Processed_Installed_Capacity_Germany_Scalers,\n",
    "        \"Processed_Prices_Europe_Scalers\": Processed_Prices_Europe_Scalers,\n",
    "        \"Processed_Realised_Supply_Germany_Scalers\": Processed_Realised_Supply_Germany_Scalers,\n",
    "        \"Processed_Realised_Demand_Germany_Scalers\": Processed_Realised_Demand_Germany_Scalers,\n",
    "        \"Processed_Weather_Data_Germany_Scalers\": Processed_Weather_Data_Germany_Scalers,\n",
    "    }\n",
    "\n",
    "    # Ensure the directory exists where scalers will be stored\n",
    "    scalers_dir = CONF.data.preprocessed_data_dir\n",
    "    os.makedirs(scalers_dir, exist_ok=True)\n",
    "\n",
    "    # Serialize and save each scaler to a file\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        scaler_path = os.path.join(scalers_dir, f\"{scaler_name}.pkl\")\n",
    "        with open(scaler_path, \"wb\") as file:\n",
    "            pickle.dump(scaler, file)\n",
    "        print(f\"Scaler stored: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge data into a single frame and store everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.process_raw_data:\n",
    "\n",
    "    def add_prefix(df, prefix):\n",
    "        \"\"\"\n",
    "        Add a prefix to all column names except the merge key.\n",
    "        \"\"\"\n",
    "        return df.rename(\n",
    "            columns={\n",
    "                col: prefix + col if col not in io.DATE_COLUMNS else col\n",
    "                for col in df.columns\n",
    "            }\n",
    "        )\n",
    "\n",
    "    Suffix_Processed_Prices_Europe = add_prefix(Processed_Prices_Europe, \"prices_\")\n",
    "    Suffix_Processed_Installed_Capacity_Germany = add_prefix(\n",
    "        Processed_Installed_Capacity_Germany, \"capacity_\"\n",
    "    )\n",
    "    Suffix_Processed_Realised_Supply_Germany = add_prefix(\n",
    "        Processed_Realised_Supply_Germany, \"supply_\"\n",
    "    )\n",
    "    Suffix_Processed_Realised_Demand_Germany = add_prefix(\n",
    "        Processed_Realised_Demand_Germany, \"demand_\"\n",
    "    )\n",
    "    Suffix_Processed_Weather_Data_Germany = add_prefix(\n",
    "        Processed_Weather_Data_Germany, \"weather_\"\n",
    "    )\n",
    "    Suffix_Processed_Weather_Data_Germany = (\n",
    "        Suffix_Processed_Weather_Data_Germany.rename(\n",
    "            columns={\"weather_time\": \"Date to\"}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Now perform the merge\n",
    "    df = pd.merge(\n",
    "        Suffix_Processed_Prices_Europe,\n",
    "        Suffix_Processed_Installed_Capacity_Germany,\n",
    "        on=io.DATE_COLUMNS,\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        df, Suffix_Processed_Realised_Supply_Germany, on=io.DATE_COLUMNS, how=\"inner\"\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        df, Suffix_Processed_Realised_Demand_Germany, on=io.DATE_COLUMNS, how=\"inner\"\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        df, Suffix_Processed_Weather_Data_Germany, on=io.DATE_COLUMNS[-1], how=\"inner\"\n",
    "    )\n",
    "    df = preprocess.split_data(df=df, column_name=io.DATE_COLUMNS[-1])\n",
    "    df.to_csv(os.path.join(CONF.data.preprocessed_data_dir, \"df.csv\"), index=False)\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate profile reports\n",
    "\n",
    "if CONF.pipeline.data_test and CONF.pipeline.inspect:\n",
    "    data.save_data_inspection(\n",
    "        Installed_Capacity_Germany=Processed_Installed_Capacity_Germany,\n",
    "        Prices_Europe=Processed_Prices_Europe,\n",
    "        Realised_Supply_Germany=Processed_Realised_Supply_Germany,\n",
    "        Realised_Demand_Germany=Processed_Realised_Demand_Germany,\n",
    "        Weather_Data_Germany=Processed_Weather_Data_Germany,\n",
    "        CONF=CONF,\n",
    "        data_type=\"preprocessed\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.data_test and CONF.pipeline.plot:\n",
    "    plots.plot_df(\n",
    "        Processed_Installed_Capacity_Germany, \"Installed_Capacity_Germany\", CONF\n",
    "    )\n",
    "    plots.plot_df(Processed_Prices_Europe, \"Prices_Europe\", CONF)\n",
    "    plots.plot_df(Processed_Realised_Supply_Germany, \"Realised_Supply_Germany\", CONF)\n",
    "    plots.plot_df(Processed_Realised_Demand_Germany, \"Realised_Demand_Germany\", CONF)\n",
    "    plots.plot_df(\n",
    "        Processed_Weather_Data_Germany,\n",
    "        \"Weather_Data_Germany\",\n",
    "        CONF,\n",
    "        date_col=io.DATE_COLUMNS_WEATHER[-1],\n",
    "        drop_date_cols=io.DATE_COLUMNS_WEATHER,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect time's resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.data_test:\n",
    "    inspection.date_range_and_resolution_dfs(\n",
    "        Installed_Capacity_Germany=Processed_Installed_Capacity_Germany,\n",
    "        Prices_Europe=Processed_Prices_Europe,\n",
    "        Realised_Supply_Germany=Processed_Realised_Supply_Germany,\n",
    "        Realised_Demand_Germany=Processed_Realised_Demand_Germany,\n",
    "        Weather_Data_Germany=Processed_Weather_Data_Germany,\n",
    "        processed=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.data_test:\n",
    "    dfs = [\n",
    "        Processed_Installed_Capacity_Germany,\n",
    "        Processed_Prices_Europe,\n",
    "        Processed_Realised_Supply_Germany,\n",
    "        Processed_Realised_Demand_Germany,\n",
    "    ]\n",
    "\n",
    "    raw_dfs = [\n",
    "        Installed_Capacity_Germany_Raw,\n",
    "        Prices_Europe_Raw,\n",
    "        Realised_Supply_Germany_Raw,\n",
    "        Realised_Demand_Germany_Raw,\n",
    "    ]\n",
    "\n",
    "    # assert all data frames have same length.\n",
    "    def test_dataframe_lengths(dfs):\n",
    "        expected_length = len(dfs[0])\n",
    "        for df in dfs[1:]:\n",
    "            assert len(df) == expected_length, \"DataFrames have different lengths\"\n",
    "\n",
    "    test_dataframe_lengths(dfs)\n",
    "\n",
    "    # assert all data frames have have same time resolution\n",
    "    def test_dataframe_resolutions(dfs):\n",
    "        date_column = \"Date from\"\n",
    "        expected_resolution = dfs[0][date_column].diff().dropna().mode()[0]\n",
    "        for df in dfs[1:]:\n",
    "            current_resolution = df[date_column].diff().dropna().mode()[0]\n",
    "            assert (\n",
    "                current_resolution == expected_resolution\n",
    "            ), \"DataFrames have different time resolutions\"\n",
    "        current_resolution = (\n",
    "            Processed_Weather_Data_Germany[\"time\"].diff().dropna().mode()[0]\n",
    "        )\n",
    "        assert (\n",
    "            current_resolution == expected_resolution\n",
    "        ), \"DataFrames have different time resolutions\"\n",
    "\n",
    "    test_dataframe_resolutions(dfs)\n",
    "\n",
    "    # Assert every row of every df has the same \"Date to\"\n",
    "    def test_date_to_consistency(dfs):\n",
    "        date_to_values = dfs[0][\"Date to\"].values\n",
    "        for i, df in enumerate(dfs[1:]):\n",
    "            assert all(\n",
    "                df[\"Date to\"].values == date_to_values\n",
    "            ), f\"Mismatch in 'Date to' values across DataFrames {i + 1}\"\n",
    "        assert np.all(\n",
    "            Processed_Weather_Data_Germany[\"time\"].values == date_to_values\n",
    "        ), \"DataFrames have different time resolutions\"\n",
    "\n",
    "    test_date_to_consistency(dfs)\n",
    "\n",
    "    # assert that not cell is missed\n",
    "    def test_no_missing_cells(dfs):\n",
    "        for i, df in enumerate(dfs + [Processed_Weather_Data_Germany]):\n",
    "            for column in df.columns:\n",
    "                assert (\n",
    "                    df[column].isnull().sum() == 0\n",
    "                ), f\"Missing values found in column '{column}' of DataFrame at index {i}\"\n",
    "\n",
    "    test_no_missing_cells(dfs)\n",
    "\n",
    "    # assert that raw dfs and normal dfs have same set of columns\n",
    "    def test_same_columns(raw_dfs, processed_dfs, ignore_columns=None):\n",
    "        if ignore_columns is None:\n",
    "            ignore_columns = set()\n",
    "\n",
    "        for raw_df, processed_df in zip(raw_dfs, processed_dfs):\n",
    "            raw_columns = set(raw_df.columns) - ignore_columns\n",
    "            processed_columns = set(processed_df.columns) - ignore_columns\n",
    "\n",
    "            assert (\n",
    "                raw_columns == processed_columns\n",
    "            ), f\"Column mismatch between raw and processed DataFrames. Missing {raw_columns - processed_columns} or {processed_columns - raw_columns}\"\n",
    "\n",
    "    # Example usage of the function, ignoring 'train', 'val', and 'test'\n",
    "    test_same_columns(\n",
    "        raw_dfs,\n",
    "        dfs,\n",
    "        ignore_columns={\n",
    "            \"∅ Neighbouring DE/LU [€/MWh]\",\n",
    "            \"Hungary [€/MWh]\",\n",
    "            \"Poland [€/MWh]\",\n",
    "            \"DE/AT/LU [€/MWh]\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity test plot final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.data_test and CONF.pipeline.plot:\n",
    "    plots.plot_df(df, \"Final Dataframe\", CONF, figsize=(150, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(CONF.data.preprocessed_data_dir, \"df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.do_hyperopt:\n",
    "    hyperopt.hyper_parameter_optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with best hyperparameters on train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.do_final_train:\n",
    "    best_hyperparameters = pickle.load(open(CONF.model.best_hyperparameter_path, \"rb\"))\n",
    "    training.train_loop(best_hyperparameters, df, \"best\", merge_train_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = pickle.load(open(CONF.model.best_hyperparameter_path, \"rb\"))\n",
    "\n",
    "if CONF.pipeline.do_test:\n",
    "    model = TimeSeriesTransformer(hyperparameters=best_hyperparameters).to(\n",
    "        device\n",
    "    )  # Adjust parameters as necessary\n",
    "    model = model.to(device)\n",
    "    model_state_dict = torch.load(CONF.model.final_model_path)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    test_df = df[df[\"test\"]].reset_index()\n",
    "    test_dataset = TimeSeriesDataset(test_df, hyperparameters=best_hyperparameters)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=best_hyperparameters.train.batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            ground_truths.append(targets.cpu().numpy())\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "    ground_truths = np.concatenate(ground_truths)\n",
    "    predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale predictions and ground truth back to original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.do_test:\n",
    "    # Define the path to the scaler file\n",
    "    scaler_path = os.path.join(\n",
    "        best_hyperparameters.data.preprocessed_data_dir,\n",
    "        \"Processed_Realised_Supply_Germany_Scalers.pkl\",\n",
    "    )\n",
    "\n",
    "    # Load the scaler from the file\n",
    "    with open(scaler_path, \"rb\") as file:\n",
    "        Processed_Realised_Supply_Germany_Scalers = pickle.load(file)\n",
    "\n",
    "    # Invert the normalization\n",
    "    for tidx, target in enumerate(best_hyperparameters.model.targets):\n",
    "        scaler = Processed_Realised_Supply_Germany_Scalers[target.split(\"_\")[-1]]\n",
    "        predictions[:, :, tidx] = scaler.inverse_transform(predictions[:, :, tidx])\n",
    "        ground_truths[:, :, tidx] = scaler.inverse_transform(ground_truths[:, :, tidx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute test loss on scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = best_hyperparameters.train.loss()\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for preds, targets in tqdm(zip(predictions, ground_truths)):\n",
    "        loss = criterion(torch.tensor(preds), torch.tensor(targets))\n",
    "        losses.append(loss.item())\n",
    "print(f\"Mean Squared Error: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming CONF.test.do_test is checked somewhere else in your code\n",
    "if CONF.pipeline.do_test:\n",
    "    _, num_horizons, num_targets = ground_truths.shape\n",
    "\n",
    "    # Create a figure with subplots arranged in 2 rows and 7 columns\n",
    "    fig, axes = plt.subplots(\n",
    "        num_horizons, num_targets, figsize=(54, 18)\n",
    "    )  # Adjust the figsize to ensure adequate spacing\n",
    "\n",
    "    # Loop over each horizon and target combination\n",
    "    for h in range(num_horizons):\n",
    "        for t in range(num_targets):\n",
    "            ax = axes[h, t]  # Locate the right subplot\n",
    "            ax.plot(\n",
    "                ground_truths[:, h, t], label=\"Ground Truth\"\n",
    "            )  # Plot ground truth for horizon h, target t\n",
    "            ax.plot(\n",
    "                predictions[:, h, t], label=\"Predictions\"\n",
    "            )  # Plot predictions for horizon h, target t\n",
    "            ax.set_title(\n",
    "                f\"Horizon {best_hyperparameters.horizons[h]}, Target {CONF.model.targets[t]}\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Time\")\n",
    "            ax.set_ylabel(\"Value\")\n",
    "            ax.legend()\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.pipeline.do_test:\n",
    "    _, num_horizons, num_targets = ground_truths.shape\n",
    "\n",
    "    # Create a figure with subplots arranged in 2 rows and 7 columns\n",
    "    fig, axes = plt.subplots(\n",
    "        num_horizons, num_targets, figsize=(54, 18)\n",
    "    )  # Adjust the figsize to ensure adequate spacing\n",
    "\n",
    "    # Loop over each horizon and target combination\n",
    "    for h in range(num_horizons):\n",
    "        for t in range(num_targets):\n",
    "            ax = axes[h, t]  # Locate the right subplot\n",
    "            residuals = (\n",
    "                predictions[:, h, t] - ground_truths[:, h, t]\n",
    "            )  # Calculate residuals\n",
    "            ax.hist(\n",
    "                residuals,\n",
    "                bins=30,\n",
    "                alpha=0.75,\n",
    "                color=\"blue\",  # Choose bin count and style\n",
    "            )\n",
    "            ax.set_title(\n",
    "                f\"Residuals Histogram for Horizon {best_hyperparameters.horizons[h]}, Target {CONF.model.targets[t]}\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Residuals\")\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "            ax.grid(True)  # Optional: Adds grid for better readability\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()  # Display the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-energy-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
