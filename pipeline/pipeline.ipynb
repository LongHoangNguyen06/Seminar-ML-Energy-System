{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from pipeline import data\n",
    "from pipeline.config import CONF\n",
    "from pipeline.data import plots\n",
    "from pipeline.data import io\n",
    "from pipeline.data import inspection\n",
    "from pipeline.data import preprocess\n",
    "\n",
    "# To suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# black is a code formatter (see https://github.com/psf/black).\n",
    "# It will automatically format the code you write in the cells imposing consistent Python style.\n",
    "%load_ext jupyter_black\n",
    "# matplotlib style file\n",
    "# Template for style file: https://matplotlib.org/stable/tutorials/introductory/customizing.html#customizing-with-style-sheets\n",
    "plt.style.use(\"../matplotlib_style.txt\")\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.expand_frame_repr\", False)  # Prevent wrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data\n",
    "\n",
    "This takes about 1 minute for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.process_raw_data and not CONF.data.loaded_raw_data:\n",
    "    # Load raw data\n",
    "    (\n",
    "        Installed_Capacity_Germany_Raw,\n",
    "        Prices_Europe_Raw,\n",
    "        Realised_Supply_Germany_Raw,\n",
    "        Realised_Demand_Germany_Raw,\n",
    "        Weather_Data_Germany_Raw,\n",
    "        Weather_Data_Germany_2022_Raw,\n",
    "    ) = data.load_data(CONF=CONF)\n",
    "    CONF.data.loaded_raw_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.process_raw_data:\n",
    "    Installed_Capacity_Germany = Installed_Capacity_Germany_Raw.copy()\n",
    "    Prices_Europe = Prices_Europe_Raw.copy()\n",
    "    Realised_Supply_Germany = Realised_Supply_Germany_Raw.copy()\n",
    "    Realised_Demand_Germany = Realised_Demand_Germany_Raw.copy()\n",
    "    Weather_Data_Germany = Weather_Data_Germany_Raw.copy()\n",
    "    Weather_Data_Germany_2022 = Weather_Data_Germany_2022_Raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.process_raw_data:\n",
    "    # Generate profile reports\n",
    "    if CONF.data.inspect:\n",
    "        data.save_data_inspection(\n",
    "            Installed_Capacity_Germany=Installed_Capacity_Germany,\n",
    "            Prices_Europe=Prices_Europe,\n",
    "            Realised_Supply_Germany=Realised_Supply_Germany,\n",
    "            Realised_Demand_Germany=Realised_Demand_Germany,\n",
    "            Weather_Data_Germany=Weather_Data_Germany,\n",
    "            Weather_Data_Germany_2022=Weather_Data_Germany_2022,\n",
    "            CONF=CONF,\n",
    "            data_type=\"raw\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.process_raw_data:\n",
    "    inspection.date_range_and_resolution_dfs(\n",
    "        Installed_Capacity_Germany=Installed_Capacity_Germany,\n",
    "        Prices_Europe=Prices_Europe,\n",
    "        Realised_Supply_Germany=Realised_Supply_Germany,\n",
    "        Realised_Demand_Germany=Realised_Demand_Germany,\n",
    "        Weather_Data_Germany=Weather_Data_Germany,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.plot:\n",
    "    plots.plot_df(\n",
    "        Installed_Capacity_Germany,\n",
    "        \"Installed_Capacity_Germany\",\n",
    "        CONF,\n",
    "        processed_data=False,\n",
    "    )\n",
    "    plots.plot_df(Prices_Europe, \"Prices_Europe\", CONF, processed_data=False)\n",
    "    plots.plot_df(\n",
    "        Realised_Supply_Germany, \"Realised_Supply_Germany\", CONF, processed_data=False\n",
    "    )\n",
    "    plots.plot_df(\n",
    "        Realised_Demand_Germany, \"Realised_Demand_Germany\", CONF, processed_data=False\n",
    "    )\n",
    "    plots.plot_df(\n",
    "        Weather_Data_Germany,\n",
    "        \"Weather_Data_Germany\",\n",
    "        CONF,\n",
    "        date_col=io.DATE_COLUMNS_WEATHER[-1],\n",
    "        drop_date_cols=io.DATE_COLUMNS_WEATHER,\n",
    "        processed_data=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging weather data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.process_raw_data:\n",
    "    # Remove the data for 2022 from the original dataframe\n",
    "    Weather_Data_Germany = Weather_Data_Germany[\n",
    "        Weather_Data_Germany[\"time\"].dt.year != 2022\n",
    "    ]\n",
    "\n",
    "    # Concatenate the filtered original dataframe with the 2022 data\n",
    "    Weather_Data_Germany = pd.concat(\n",
    "        [Weather_Data_Germany, Weather_Data_Germany_2022], ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.process_raw_data:\n",
    "    Processed_Installed_Capacity_Germany = data.process_na_values(\n",
    "        Installed_Capacity_Germany, CONF\n",
    "    )\n",
    "    Processed_Prices_Europe = data.process_na_values(Prices_Europe, CONF)\n",
    "    Processed_Realised_Supply_Germany = data.process_na_values(\n",
    "        Realised_Supply_Germany, CONF\n",
    "    )\n",
    "    Processed_Realised_Demand_Germany = data.process_na_values(\n",
    "        Realised_Demand_Germany, CONF\n",
    "    )\n",
    "    Processed_Weather_Data_Germany = data.process_na_values(Weather_Data_Germany, CONF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.process_raw_data:\n",
    "    Processed_Weather_Data_Germany = preprocess.aggregate_weather_data(\n",
    "        Processed_Weather_Data_Germany, [\"forecast_origin\", \"time\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decrease demand and supply's time resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.process_raw_data:\n",
    "    Processed_Realised_Demand_Germany = Processed_Realised_Demand_Germany[\n",
    "        Processed_Realised_Demand_Germany[\"Date to\"].dt.minute == 0\n",
    "    ]\n",
    "    Processed_Realised_Demand_Germany[\"Date from\"] = Processed_Realised_Demand_Germany[\n",
    "        \"Date to\"\n",
    "    ] - pd.Timedelta(hours=1)\n",
    "    Processed_Realised_Supply_Germany = Processed_Realised_Supply_Germany[\n",
    "        Processed_Realised_Supply_Germany[\"Date to\"].dt.minute == 0\n",
    "    ]\n",
    "    Processed_Realised_Supply_Germany[\"Date from\"] = Processed_Realised_Supply_Germany[\n",
    "        \"Date to\"\n",
    "    ] - pd.Timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase time resolution of capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\n",
    "    \"Date from\": pd.Timestamp(\"2022-12-31 23:00:00\"),\n",
    "    \"Date to\": pd.Timestamp(\"2023-01-01 00:00:00\"),\n",
    "}\n",
    "new_row_df = pd.DataFrame([new_row])\n",
    "Processed_Installed_Capacity_Germany_hourly = pd.concat(\n",
    "    [Processed_Installed_Capacity_Germany, new_row_df], ignore_index=True\n",
    ")\n",
    "\n",
    "Processed_Installed_Capacity_Germany_hourly = (\n",
    "    Processed_Installed_Capacity_Germany_hourly.set_index(\"Date from\")\n",
    ")\n",
    "Processed_Installed_Capacity_Germany_hourly = (\n",
    "    Processed_Installed_Capacity_Germany_hourly.resample(\"H\").mean()\n",
    ")\n",
    "Processed_Installed_Capacity_Germany_hourly.reset_index(inplace=True)\n",
    "Processed_Installed_Capacity_Germany_hourly[\"Date to\"] = (\n",
    "    Processed_Installed_Capacity_Germany_hourly[\"Date from\"] + pd.Timedelta(hours=1)\n",
    ")\n",
    "Processed_Installed_Capacity_Germany = Processed_Installed_Capacity_Germany_hourly\n",
    "Processed_Installed_Capacity_Germany = Processed_Installed_Capacity_Germany.fillna(\n",
    "    method=\"ffill\"\n",
    ")\n",
    "inspection.date_range_and_resolution(\n",
    "    Processed_Installed_Capacity_Germany, io.DATE_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim rows of every df to have same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim first row of Processed_Weather_Data_Germany\n",
    "Processed_Weather_Data_Germany = Processed_Weather_Data_Germany[\n",
    "    Processed_Weather_Data_Germany[\"time\"]\n",
    "    != Processed_Weather_Data_Germany[\"time\"].min()\n",
    "]\n",
    "\n",
    "# trim last row of every other df\n",
    "Processed_Installed_Capacity_Germany = Processed_Installed_Capacity_Germany[\n",
    "    Processed_Installed_Capacity_Germany[\"Date to\"]\n",
    "    != Processed_Installed_Capacity_Germany[\"Date to\"].max()\n",
    "]\n",
    "Processed_Prices_Europe = Processed_Prices_Europe[\n",
    "    Processed_Prices_Europe[\"Date to\"] != Processed_Prices_Europe[\"Date to\"].max()\n",
    "]\n",
    "Processed_Realised_Supply_Germany = Processed_Realised_Supply_Germany[\n",
    "    Processed_Realised_Supply_Germany[\"Date to\"]\n",
    "    != Processed_Realised_Supply_Germany[\"Date to\"].max()\n",
    "]\n",
    "Processed_Realised_Demand_Germany = Processed_Realised_Demand_Germany[\n",
    "    Processed_Realised_Demand_Germany[\"Date to\"]\n",
    "    != Processed_Realised_Demand_Germany[\"Date to\"].max()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patch time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Prices_Europe = preprocess.patch_time_saving(Processed_Prices_Europe)\n",
    "Processed_Realised_Demand_Germany = preprocess.patch_time_saving(\n",
    "    Processed_Realised_Demand_Germany\n",
    ")\n",
    "Processed_Realised_Supply_Germany = preprocess.patch_time_saving(\n",
    "    Processed_Realised_Supply_Germany\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.process_raw_data and CONF.data.normalize_data:\n",
    "    print(\"Split data in train, val and test\")\n",
    "    Processed_Installed_Capacity_Germany = preprocess.split_data(\n",
    "        df=Processed_Installed_Capacity_Germany, column_name=io.DATE_COLUMNS[-1]\n",
    "    )\n",
    "    Processed_Prices_Europe = preprocess.split_data(\n",
    "        df=Processed_Prices_Europe, column_name=io.DATE_COLUMNS[-1]\n",
    "    )\n",
    "    Processed_Realised_Supply_Germany = preprocess.split_data(\n",
    "        df=Processed_Realised_Supply_Germany, column_name=io.DATE_COLUMNS[-1]\n",
    "    )\n",
    "    Processed_Realised_Demand_Germany = preprocess.split_data(\n",
    "        df=Processed_Realised_Demand_Germany, column_name=io.DATE_COLUMNS[-1]\n",
    "    )\n",
    "    Processed_Weather_Data_Germany = preprocess.split_data(\n",
    "        df=Processed_Weather_Data_Germany, column_name=io.DATE_COLUMNS_WEATHER[0]\n",
    "    )\n",
    "\n",
    "    print(\"Normalizing data\")\n",
    "    (\n",
    "        Processed_Installed_Capacity_Germany,\n",
    "        Processed_Installed_Capacity_Germany_Scalers,\n",
    "    ) = preprocess.normalize_data(\n",
    "        df=Processed_Installed_Capacity_Germany,\n",
    "        ignore_features=io.DATE_COLUMNS,\n",
    "        constant=CONF.data.price_normalization_constant,\n",
    "    )\n",
    "\n",
    "    Processed_Prices_Europe, Processed_Prices_Europe_Scalers = (\n",
    "        preprocess.normalize_data(\n",
    "            df=Processed_Prices_Europe,\n",
    "            ignore_features=io.DATE_COLUMNS,\n",
    "            constant=CONF.data.price_normalization_constant,\n",
    "        )\n",
    "    )\n",
    "    Processed_Realised_Supply_Germany, Processed_Realised_Supply_Germany_Scalers = (\n",
    "        preprocess.normalize_data(\n",
    "            df=Processed_Realised_Supply_Germany, ignore_features=io.DATE_COLUMNS\n",
    "        )\n",
    "    )\n",
    "    Processed_Realised_Demand_Germany, Processed_Realised_Demand_Germany_Scalers = (\n",
    "        preprocess.normalize_data(\n",
    "            df=Processed_Realised_Demand_Germany, ignore_features=io.DATE_COLUMNS\n",
    "        )\n",
    "    )\n",
    "    Processed_Weather_Data_Germany, Processed_Weather_Data_Germany_Scalers = (\n",
    "        preprocess.normalize_data(\n",
    "            df=Processed_Weather_Data_Germany,\n",
    "            ignore_features=io.DATE_COLUMNS_WEATHER + [\"longitude\", \"latitude\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Remove train, test, val columns, again\")\n",
    "    Processed_Installed_Capacity_Germany = Processed_Installed_Capacity_Germany.drop(\n",
    "        [\"train\", \"val\", \"test\"], axis=1\n",
    "    )\n",
    "    Processed_Prices_Europe = Processed_Prices_Europe.drop(\n",
    "        [\"train\", \"val\", \"test\"], axis=1\n",
    "    )\n",
    "    Processed_Realised_Supply_Germany = Processed_Realised_Supply_Germany.drop(\n",
    "        [\"train\", \"val\", \"test\"], axis=1\n",
    "    )\n",
    "    Processed_Realised_Demand_Germany = Processed_Realised_Demand_Germany.drop(\n",
    "        [\"train\", \"val\", \"test\"], axis=1\n",
    "    )\n",
    "    Processed_Weather_Data_Germany = Processed_Weather_Data_Germany.drop(\n",
    "        [\"train\", \"val\", \"test\"], axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate profile reports\n",
    "\n",
    "if CONF.data.inspect:\n",
    "    data.save_data_inspection(\n",
    "        Installed_Capacity_Germany=Processed_Installed_Capacity_Germany,\n",
    "        Prices_Europe=Processed_Prices_Europe,\n",
    "        Realised_Supply_Germany=Processed_Realised_Supply_Germany,\n",
    "        Realised_Demand_Germany=Processed_Realised_Demand_Germany,\n",
    "        Weather_Data_Germany=Processed_Weather_Data_Germany,\n",
    "        CONF=CONF,\n",
    "        data_type=\"preprocessed\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.plot:\n",
    "    plots.plot_df(\n",
    "        Processed_Installed_Capacity_Germany, \"Installed_Capacity_Germany\", CONF\n",
    "    )\n",
    "    plots.plot_df(Processed_Prices_Europe, \"Prices_Europe\", CONF)\n",
    "    plots.plot_df(Processed_Realised_Supply_Germany, \"Realised_Supply_Germany\", CONF)\n",
    "    plots.plot_df(Processed_Realised_Demand_Germany, \"Realised_Demand_Germany\", CONF)\n",
    "    plots.plot_df(\n",
    "        Processed_Weather_Data_Germany,\n",
    "        \"Weather_Data_Germany\",\n",
    "        CONF,\n",
    "        date_col=io.DATE_COLUMNS_WEATHER[-1],\n",
    "        drop_date_cols=io.DATE_COLUMNS_WEATHER,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect time's resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspection.date_range_and_resolution_dfs(\n",
    "    Installed_Capacity_Germany=Processed_Installed_Capacity_Germany,\n",
    "    Prices_Europe=Processed_Prices_Europe,\n",
    "    Realised_Supply_Germany=Processed_Realised_Supply_Germany,\n",
    "    Realised_Demand_Germany=Processed_Realised_Demand_Germany,\n",
    "    Weather_Data_Germany=Processed_Weather_Data_Germany,\n",
    "    processed=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    Processed_Installed_Capacity_Germany,\n",
    "    Processed_Prices_Europe,\n",
    "    Processed_Realised_Supply_Germany,\n",
    "    Processed_Realised_Demand_Germany,\n",
    "]\n",
    "\n",
    "raw_dfs = [\n",
    "    Installed_Capacity_Germany_Raw,\n",
    "    Prices_Europe_Raw,\n",
    "    Realised_Supply_Germany_Raw,\n",
    "    Realised_Demand_Germany_Raw,\n",
    "]\n",
    "\n",
    "\n",
    "# assert all data frames have same length.\n",
    "def test_dataframe_lengths(dfs):\n",
    "    expected_length = len(dfs[0])\n",
    "    for df in dfs[1:]:\n",
    "        assert len(df) == expected_length, \"DataFrames have different lengths\"\n",
    "\n",
    "\n",
    "test_dataframe_lengths(dfs)\n",
    "\n",
    "\n",
    "# assert all data frames have have same time resolution\n",
    "def test_dataframe_resolutions(dfs):\n",
    "    date_column = \"Date from\"\n",
    "    expected_resolution = dfs[0][date_column].diff().dropna().mode()[0]\n",
    "    for df in dfs[1:]:\n",
    "        current_resolution = df[date_column].diff().dropna().mode()[0]\n",
    "        assert (\n",
    "            current_resolution == expected_resolution\n",
    "        ), \"DataFrames have different time resolutions\"\n",
    "    current_resolution = (\n",
    "        Processed_Weather_Data_Germany[\"time\"].diff().dropna().mode()[0]\n",
    "    )\n",
    "    assert (\n",
    "        current_resolution == expected_resolution\n",
    "    ), \"DataFrames have different time resolutions\"\n",
    "\n",
    "\n",
    "test_dataframe_resolutions(dfs)\n",
    "\n",
    "\n",
    "# Assert every row of every df has the same \"Date to\"\n",
    "def test_date_to_consistency(dfs):\n",
    "    date_to_values = dfs[0][\"Date to\"].values\n",
    "    for i, df in enumerate(dfs[1:]):\n",
    "        assert all(\n",
    "            df[\"Date to\"].values == date_to_values\n",
    "        ), f\"Mismatch in 'Date to' values across DataFrames {i + 1}\"\n",
    "    assert np.all(\n",
    "        Processed_Weather_Data_Germany[\"time\"].values == date_to_values\n",
    "    ), \"DataFrames have different time resolutions\"\n",
    "\n",
    "\n",
    "test_date_to_consistency(dfs)\n",
    "\n",
    "\n",
    "# assert that not cell is missed\n",
    "def test_no_missing_cells(dfs):\n",
    "    for i, df in enumerate(dfs + [Processed_Weather_Data_Germany]):\n",
    "        for column in df.columns:\n",
    "            assert (\n",
    "                df[column].isnull().sum() == 0\n",
    "            ), f\"Missing values found in column '{column}' of DataFrame at index {i}\"\n",
    "\n",
    "\n",
    "test_no_missing_cells(dfs)\n",
    "\n",
    "\n",
    "# assert that raw dfs and normal dfs have same set of columns\n",
    "def test_same_columns(raw_dfs, processed_dfs, ignore_columns=None):\n",
    "    if ignore_columns is None:\n",
    "        ignore_columns = set()\n",
    "\n",
    "    for raw_df, processed_df in zip(raw_dfs, processed_dfs):\n",
    "        raw_columns = set(raw_df.columns) - ignore_columns\n",
    "        processed_columns = set(processed_df.columns) - ignore_columns\n",
    "\n",
    "        assert (\n",
    "            raw_columns == processed_columns\n",
    "        ), f\"Column mismatch between raw and processed DataFrames. Missing {raw_columns - processed_columns} or {processed_columns - raw_columns}\"\n",
    "\n",
    "\n",
    "# Example usage of the function, ignoring 'train', 'val', and 'test'\n",
    "test_same_columns(\n",
    "    raw_dfs,\n",
    "    dfs,\n",
    "    ignore_columns={\n",
    "        \"∅ Neighbouring DE/LU [€/MWh]\",\n",
    "        \"Hungary [€/MWh]\",\n",
    "        \"Poland [€/MWh]\",\n",
    "        \"DE/AT/LU [€/MWh]\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader for supply forecasting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge data together to a single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(df, prefix):\n",
    "    \"\"\"\n",
    "    Add a prefix to all column names except the merge key.\n",
    "    \"\"\"\n",
    "    return df.rename(\n",
    "        columns={\n",
    "            col: prefix + col if col not in io.DATE_COLUMNS else col\n",
    "            for col in df.columns\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "Suffix_Processed_Prices_Europe = add_prefix(Processed_Prices_Europe, \"prices_\")\n",
    "Suffix_Processed_Installed_Capacity_Germany = add_prefix(\n",
    "    Processed_Installed_Capacity_Germany, \"capacity_\"\n",
    ")\n",
    "Suffix_Processed_Realised_Supply_Germany = add_prefix(\n",
    "    Processed_Realised_Supply_Germany, \"supply_\"\n",
    ")\n",
    "Suffix_Processed_Realised_Demand_Germany = add_prefix(\n",
    "    Processed_Realised_Demand_Germany, \"demand_\"\n",
    ")\n",
    "Suffix_Processed_Weather_Data_Germany = add_prefix(\n",
    "    Processed_Weather_Data_Germany, \"weather_\"\n",
    ")\n",
    "Suffix_Processed_Weather_Data_Germany = Suffix_Processed_Weather_Data_Germany.rename(\n",
    "    columns={\"weather_time\": \"Date to\"}\n",
    ")\n",
    "\n",
    "# Now perform the merge\n",
    "df = pd.merge(\n",
    "    Suffix_Processed_Prices_Europe,\n",
    "    Suffix_Processed_Installed_Capacity_Germany,\n",
    "    on=io.DATE_COLUMNS,\n",
    "    how=\"inner\",\n",
    ")\n",
    "df = pd.merge(\n",
    "    df, Suffix_Processed_Realised_Supply_Germany, on=io.DATE_COLUMNS, how=\"inner\"\n",
    ")\n",
    "df = pd.merge(\n",
    "    df, Suffix_Processed_Realised_Demand_Germany, on=io.DATE_COLUMNS, how=\"inner\"\n",
    ")\n",
    "df = pd.merge(\n",
    "    df, Suffix_Processed_Weather_Data_Germany, on=io.DATE_COLUMNS[-1], how=\"inner\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF.data.plot:\n",
    "    plots.plot_df(df, \"Final Dataframe\", CONF, figsize=(150, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess.split_data(df=df, column_name=io.DATE_COLUMNS[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Torch's datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from pipeline.config import CONF\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, CONF):\n",
    "        self.dataframe = df\n",
    "        self.features = CONF.model.features\n",
    "        self.targets = CONF.model.targets\n",
    "        self.lag = CONF.model.lag\n",
    "        self.horizons = CONF.model.horizons\n",
    "\n",
    "        # Determine the maximum horizon to ensure all targets can be accessed\n",
    "        self.max_horizon = max(self.horizons)\n",
    "        # Calculate total samples considering the lag and the maximum forecast horizon\n",
    "        self.total_samples = len(df) - (self.lag + self.max_horizon)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Adjust start index to accommodate the lag\n",
    "        idx += self.lag\n",
    "\n",
    "        # Collect inputs using the lag\n",
    "        inputs = torch.tensor(\n",
    "            self.dataframe.loc[\n",
    "                [idx - lag for lag in range(self.lag)], self.features\n",
    "            ].values.astype(float)\n",
    "        )  # Shape: [n_batches, n_lag, n_features]\n",
    "        # Collect targets for each horizon and each feature\n",
    "        targets = torch.tensor(\n",
    "            self.dataframe.loc[\n",
    "                [idx + horizon for horizon in self.horizons], self.targets\n",
    "            ].values.astype(float)\n",
    "        )  # Shape will be [n_batches, n_horizons, n_outputs]\n",
    "        return inputs.to(torch.float32), targets.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch's dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.config import CONF\n",
    "\n",
    "train_df = df[df[\"train\"]].drop(CONF.model.ignore_columns, axis=1).reset_index()\n",
    "val_df = df[df[\"val\"]].drop(CONF.model.ignore_columns, axis=1).reset_index()\n",
    "test_df = df[df[\"test\"]].drop(CONF.model.ignore_columns, axis=1).reset_index()\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_df, CONF=CONF)\n",
    "val_dataset = TimeSeriesDataset(val_df, CONF=CONF)\n",
    "test_dataset = TimeSeriesDataset(test_df, CONF=CONF)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONF.train.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONF.train.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONF.train.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from pipeline.config import CONF\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, CONF=CONF):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        num_layers = CONF.model.num_layers\n",
    "        num_heads = CONF.model.num_heads\n",
    "        forward_expansion = CONF.model.forward_expansion\n",
    "        dropout = CONF.model.dropout\n",
    "        output_horizons = CONF.model.horizons\n",
    "        d_model = CONF.model.num_features * forward_expansion\n",
    "        self.output_horizons = output_horizons\n",
    "\n",
    "        # Linear transformation to project input features to a higher dimensional space\n",
    "        self.feature_to_embedding = nn.Linear(CONF.model.num_features, d_model)\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Output layer for each horizon and feature\n",
    "        self.fc_out = nn.ModuleList(\n",
    "            [nn.Linear(d_model, CONF.model.num_targets) for _ in output_horizons]\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.permute(\n",
    "            1, 0, 2\n",
    "        )  # Permute to (sequence_length, batch_size, num_features)\n",
    "        src = self.feature_to_embedding(\n",
    "            src\n",
    "        )  # Map features to the higher dimensional space\n",
    "        src = self.positional_encoder(src)\n",
    "        transformed = self.transformer_encoder(src)\n",
    "\n",
    "        last_output = transformed[-1]  # Use only the last output for forecasting\n",
    "\n",
    "        outputs = [fc(last_output) for fc in self.fc_out]\n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainining loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from pipeline.config import CONF\n",
    "from tqdm import tqdm\n",
    "\n",
    "if CONF.train.do_train:\n",
    "    # Assuming the model and dataset classes are already imported and configured\n",
    "    model = TimeSeriesTransformer(CONF=CONF)\n",
    "    optimizer = Adam(model.parameters(), lr=CONF.train.lr)\n",
    "    criterion = (\n",
    "        nn.MSELoss()\n",
    "    )  # You can change the loss function based on your specific needs\n",
    "\n",
    "    # Training loop\n",
    "    def train(model, train_loader, optimizer, criterion, device):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "        progress_bar.close()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    def validate(model, val_loader, criterion, device):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in progress_bar:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({'batch_loss': f'{loss.item():.4f}'})\n",
    "        progress_bar.close()\n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Main training loop\n",
    "    num_epochs = CONF.train.epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-energy-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
